{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Traffic Anomaly Detection Using Recurrent Neural Networks\n",
    "## Replication File 4 of 6\n",
    "\n",
    "Benjamin J. Radford, Leonardo M. Apolonio, Antonio J. Trias, and Jim A. Simpson\n",
    "\n",
    "Paper available: [arXiv:1803.10769v1](https://arxiv.org/pdf/1803.10769.pdf).\n",
    "\n",
    "DISTRIBUTION STATEMENT A: Approved for public release. \n",
    "\n",
    "This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a9e50a63a4fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMasking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import re\n",
    "import h5py\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import log_loss, auc, roc_curve\n",
    "from keras.layers.core import Masking\n",
    "from keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, TimeDistributed\n",
    "from tensorflow.python.client import device_lib\n",
    "from lxml import etree\n",
    "from itertools import groupby\n",
    "from gensim.models import Word2Vec\n",
    "import glob\n",
    "import math\n",
    "import itertools\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##\n",
    "## Load data\n",
    "##\n",
    "print(\"Loading data...\")\n",
    "xml_list = glob.glob('data/labeled_flows_xml/*xml')\n",
    "\n",
    "parser = etree.XMLParser(recover=True)\n",
    "\n",
    "def xml2df(xml_data):\n",
    "    root = etree.fromstring(xml_data, parser=parser) # element tree\n",
    "    all_records = []\n",
    "    for i, child in enumerate(root):\n",
    "        record = {}\n",
    "        for subchild in child:\n",
    "            record[subchild.tag] = subchild.text\n",
    "            all_records.append(record)\n",
    "    return pandas.DataFrame(all_records)\n",
    "\n",
    "dfs = []\n",
    "for ii in xml_list:\n",
    "    xml_data = open(ii).read()\n",
    "    dfs.append(xml2df(xml_data))\n",
    "    \n",
    "data = pandas.concat(dfs)\n",
    "data = data.drop_duplicates()\n",
    "data = data.sort_values(\"startDateTime\")\n",
    "del dfs\n",
    "\n",
    "##\n",
    "## Create IP-dyad hours\n",
    "##\n",
    "print(\"De-dup Flows: \"+str(len(data)))\n",
    "data = data.sort_values('startDateTime')\n",
    "data['totalBytes'] = data.totalSourceBytes.astype(float) + data.totalDestinationBytes.astype(float)\n",
    "data['lowIP'] = data[['source','destination']].apply(lambda x: x[0] if x[0] <= x[1] else x[1], axis=1)\n",
    "data['highIP'] = data[['source','destination']].apply(lambda x: x[0] if x[0] > x[1] else x[1], axis=1)\n",
    "data['seqId'] = data['lowIP'] + '_' + data['highIP']  + '_' + data['startDateTime'].str[:13]\n",
    "data['protoBytes'] = data[['protocolName','totalBytes']].apply(lambda x: str(x[0])[0] + str(math.floor(np.log2(x[1] + 1.0))), axis=1)\n",
    "\n",
    "##\n",
    "## Group by key and produce sequences\n",
    "## \n",
    "key = data.groupby('seqId')[['Tag','protoBytes']].agg({\"Tag\":lambda x: \"%s\" % ','.join([a for a in x]),\"protoBytes\":lambda x: \"%s\" % ','.join([str(a) for a in x])})\n",
    "attacks = [a.split(\",\") for a in key.Tag.tolist()]\n",
    "sequences = [a.split(\",\") for a in key.protoBytes.tolist()]\n",
    "\n",
    "unique_tokens = list(set([a for b in sequences for a in b]))\n",
    "le = LabelEncoder()\n",
    "le.fit(unique_tokens)\n",
    "sequences = [le.transform(s).tolist() for s in sequences]\n",
    "sequences = [[b+1 for b in a] for a in sequences]\n",
    "\n",
    "sequence_attack = zip(attacks, sequences)\n",
    "\n",
    "##\n",
    "## Produce sequences for modeling\n",
    "##\n",
    "na_value = 0.\n",
    "seq_len = 10\n",
    "seq_index = []\n",
    "seq_x = []\n",
    "seq_y = []\n",
    "seq_attack = []\n",
    "for si, (sa, ss) in enumerate(sequence_attack):\n",
    "    prepend = [0.] * (seq_len)\n",
    "    seq =  prepend + ss\n",
    "    seqa = prepend + sa\n",
    "    for ii in range(seq_len, len(seq)):\n",
    "        subseq = seq[(ii-seq_len):(ii)]\n",
    "        vex = []\n",
    "        for ee in subseq:\n",
    "            try:\n",
    "                vex.append(ee)\n",
    "            except:\n",
    "                vex.append(na_value)\n",
    "        seq_x.append(vex)\n",
    "        seq_y.append(seq[ii])\n",
    "        seq_index.append(si)\n",
    "        seq_attack.append(seqa[ii])\n",
    "        \n",
    "##\n",
    "## Initialize One-hot-encoder\n",
    "##\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "ohe_y = ohe.fit_transform(np.asarray(seq_y).reshape(-1, 1))\n",
    "X = np.array(seq_x)\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, batch_size, x, y, ohe):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = int(math.floor(np.shape(x)[0] / batch_size))\n",
    "        self.batch_index = [a * batch_size for a in range(0, self.n_batches)]\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.ohe = ohe\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for bb in itertools.cycle(self.batch_index):\n",
    "            y = self.y[bb:(bb+self.batch_size)]\n",
    "            ohe_y = self.ohe.transform(y.reshape(len(y), 1))\n",
    "            yield (self.x[bb:(bb+self.batch_size),], ohe_y)\n",
    "            \n",
    "print(\"Ready to Go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(123)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(Masking(0., input_shape=(seq_len,w2v_size)))\n",
    "model.add(Embedding(output_dim=100, input_dim=len(unique_tokens), mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(50, activation=\"relu\", return_sequences=False)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, activation=\"linear\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(unique_tokens), activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# training_data = BatchGenerator(1000, X[np.asarray(train_index)], le_y[np.asarray(train_index)], ohe)\n",
    "\n",
    "model.fit(X, ohe_y, epochs=4, batch_size=512, verbose=1)\n",
    "\n",
    "# model.fit_generator(training_data.__iter__(),\n",
    "#     steps_per_epoch=training_data.n_batches,\n",
    "#     epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/protobytes_dirty.hd5\")\n",
    "preds = model.predict_proba(X, batch_size=512)\n",
    "test_index = range(0,len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_preds = zip(np.asarray(seq_index)[np.asarray(test_index)], preds, ohe_y[np.asarray(test_index)], np.asarray(seq_attack)[np.asarray(test_index)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='graphics/protobytes_model.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_list = []\n",
    "for (ii, pp, yy, aa) in indexed_preds:\n",
    "    ll = -math.log(pp[np.argmax(yy)])\n",
    "    logloss_list.append(ll)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(np.asarray(seq_attack)[np.asarray(test_index)],logloss_list, pos_label=\"Attack\")\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % auc(fpr,tpr))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key_ll = zip(seq_index, logloss_list, seq_attack)\n",
    "dictionary = dict()\n",
    "for (key, ll, aa) in key_ll: #\n",
    "    current_value = dictionary.get(str(key), ([],[]))\n",
    "    dictionary[str(key)] = (current_value[0] + [ll], current_value[1] + [aa])\n",
    "\n",
    "agg_ll = []\n",
    "agg_bad = []\n",
    "for key, val in dictionary.iteritems():\n",
    "    bad = str(np.mean([v==\"Attack\" for v in val[1]]) > 0.)\n",
    "    score = np.max(val[0])\n",
    "    agg_bad.append(bad)\n",
    "    agg_ll.append(score)\n",
    "    \n",
    "fpr, tpr, thresholds = roc_curve(agg_bad, agg_ll, pos_label=\"True\")\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % auc(fpr,tpr))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Proto-bytes Dirty Baseline')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"graphics/protobytes_dirty.pdf\",format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
